{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 03:33:40.840420: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 03:33:40.955511: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-12 03:33:41.572216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 03:33:41.572283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 03:33:41.572291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "from common import get_instagram_sessions, get_text_embedding, get_text_embeddings_splitted\n",
    "from collections import  defaultdict\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "import plotly.express as px\n",
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_word_lists():\n",
    "    with open('apologetic.txt', 'r') as apologetic_file:\n",
    "        apologetic_context = apologetic_file.readlines()\n",
    "    with open('comforting.txt', 'r') as comfort_file:\n",
    "        comfort_context = comfort_file.readlines()\n",
    "    with open('stopping.txt', 'r') as stopping_file:\n",
    "        stopping_context = stopping_file.readlines()\n",
    "\n",
    "    return apologetic_context, comfort_context, stopping_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing instagram_100_sessions/Compiled and Totaled Sessions 2020/CB IG Compiled Coding Set 1 - 2020 (Revised 2.0).xlsx\n",
      "Doing sheet 1 Overall CB - Yes\n",
      "Doing sheet 2 Overall CB - No\n",
      "Doing sheet 3 Overall CB - No\n",
      "Doing sheet 4 Overall CB - No\n",
      "Doing sheet 5 Overall CB - No\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "louiscabb\n",
      "(nan, nan, nan)\n",
      "So would it take a additional 7days to arrive or how much? \n",
      "nan\n",
      "Doing sheet 6 Overall CB - No\n",
      "Doing sheet 7 Overall CB - No\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "gianna_devincenzo\n",
      "(nan, nan, nan)\n",
      "@_brittany_keyes_ \n",
      "nan\n",
      "Doing sheet 8 Overall CB - Yes\n",
      "Doing sheet 9 Overall CB - No\n",
      "Doing sheet 10 Overall CB - No\n",
      "Doing sheet 11 Overall CB - No\n",
      "Doing sheet 12 Overall CB - No\n",
      "Doing sheet 13 Overall CB - Yes\n",
      "Doing sheet 14 Overall CB - No\n",
      "Doing sheet 15 Overall CB - No\n",
      "Doing sheet 16 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "elyse_turney\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "0\n",
      "Doing sheet 17 Overall CB - Yes\n",
      "Doing sheet 18 Overall CB - Yes\n",
      "Doing sheet 19 Overall CB - No\n",
      "Doing sheet 20 Overall CB - Yes\n",
      "Doing instagram_100_sessions/Compiled and Totaled Sessions 2020/CB IG Compiled Coding Set 2 - 2020 (Revised 2.0).xlsx\n",
      "Doing sheet 1 Overall CB - No\n",
      "Doing sheet 2 Overall CB - No\n",
      "Doing sheet 3 Overall CB - No\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "b.beh_\n",
      "(nan, nan, nan)\n",
      "@l.a.ris MDSS HAAAYES QUUUUE CAARA MDS VC E MUITO FOOFOFOF \n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "jordantolin\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "0\n",
      "==========\n",
      "'int' object has no attribute 'strip'\n",
      "==========\n",
      "elviralarssons\n",
      "(nan, nan, nan)\n",
      "2000\n",
      "0\n",
      "Doing sheet 4 Overall CB - No\n",
      "Doing sheet 5 Overall CB - No\n",
      "Doing sheet 6 Overall CB - No\n",
      "Doing sheet 7 Overall CB - No\n",
      "Doing sheet 8 Overall CB - No\n",
      "Doing sheet 9 Overall CB - No\n",
      "Doing sheet 10 Overall CB - No\n",
      "Doing sheet 11 Overall CB - Yes\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "jazzzzycat\n",
      "('jakekeyser', 'returndat13  ', 'returndat13  ')\n",
      "No he is an idiot. He has horrible grammar. \n",
      "nan\n",
      "Doing sheet 12 Overall CB - Yes\n",
      "Doing sheet 13 Overall CB - No\n",
      "Doing sheet 14 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 15 Overall CB - Yes\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "blkbarbieswagg\n",
      "(nan, nan, nan)\n",
      "*LIVE \n",
      "nan\n",
      "Doing sheet 16 Overall CB - No\n",
      "Doing sheet 17 Overall CB - Yes\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "lion5000\n",
      "(nan, nan, nan)\n",
      "Soft ass kirk \n",
      "nan\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "cap_chick\n",
      "(nan, nan, nan)\n",
      "@msnici _†¥__†¥Š_†_¥ï_†¥ \n",
      "nan\n",
      "Doing sheet 18 Overall CB - Yes\n",
      "Doing sheet 19 Overall CB - No\n",
      "Doing sheet 20 Overall CB - Yes\n",
      "Doing instagram_100_sessions/Compiled and Totaled Sessions 2020/CB IG Compiled Coding Set 5 - 2020 (Revised 2.0).xlsx\n",
      "Doing sheet 1 Overall CB - Yes\n",
      "Doing sheet 2 Overall CB - No\n",
      "==========\n",
      "'int' object has no attribute 'strip'\n",
      "==========\n",
      "xalexaxx_13\n",
      "(nan, nan, nan)\n",
      "44\n",
      "0\n",
      "Doing sheet 3 Overall CB - No\n",
      "Doing sheet 4 Overall CB - No\n",
      "Doing sheet 5 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 6 Overall CB - No\n",
      "Doing sheet 7 Overall CB - No\n",
      "Doing sheet 8 Overall CB - No\n",
      "Doing sheet 9 Overall CB - No\n",
      "Doing sheet 10 Overall CB - No\n",
      "Doing sheet 11 Overall CB - No\n",
      "Doing sheet 12 Overall CB - Yes\n",
      "Doing sheet 13 Overall CB - Yes\n",
      "Doing sheet 14 Overall CB - Yes\n",
      "Doing sheet 15 Overall CB - Yes\n",
      "==========\n",
      "'int' object has no attribute 'strip'\n",
      "==========\n",
      "tbhdianaa\n",
      "(nan, nan, nan)\n",
      "1\n",
      "0\n",
      "Doing sheet 16 Overall CB - Yes\n",
      "Doing sheet 17 Overall CB - Yes\n",
      "Doing sheet 18 Overall CB - Yes\n",
      "Doing sheet 19 Overall CB - Yes\n",
      "Doing sheet 20 Overall CB - No\n",
      "Doing instagram_100_sessions/Compiled and Totaled Sessions 2020/CB IG Compiled Coding Set 3 - 2020 (Revised 2.0).xlsx\n",
      "Doing sheet 1 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "3\n",
      "Doing sheet 2 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "6\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 3 Overall CB - Yes\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "disclaimerboy\n",
      "(nan, 0, nan)\n",
      "Bully Ray it's the perfect heel. He plays it so well. \n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "13\n",
      "Doing sheet 4 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "73\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 5 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "63\n",
      "Doing sheet 6 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "0\n",
      "Doing sheet 7 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "9\n",
      "Doing sheet 8 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "56\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 9 Overall CB - Yes\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "23\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "nan\n",
      "Doing sheet 10 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "1\n",
      "Doing sheet 11 Overall CB - No\n",
      "Doing sheet 12 Overall CB - No\n",
      "Doing sheet 13 Overall CB - No\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "0\n",
      "Doing sheet 14 Overall CB - No\n",
      "Doing sheet 15 Overall CB - No\n",
      "Doing sheet 16 Overall CB - No\n",
      "Doing sheet 17 Overall CB - No\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "winabob\n",
      "(nan, nan, nan)\n",
      "Both \n",
      "nan\n",
      "Doing sheet 18 Overall CB - No\n",
      "Doing sheet 19 Overall CB - Yes\n",
      "Doing sheet 20 Overall CB - No\n",
      "==========\n",
      "cannot convert float NaN to integer\n",
      "==========\n",
      "bootruetube\n",
      "(nan, nan, nan)\n",
      "WWE Battlepack 14 Randy Orton and Mason Ryan \n",
      "nan\n",
      "==========\n",
      "'float' object has no attribute 'strip'\n",
      "==========\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "nan\n",
      "0\n",
      "Doing instagram_100_sessions/Compiled and Totaled Sessions 2020/CB IG Compiled Coding Set 4 - 2020 (Revised 2.0).xlsx\n",
      "Doing sheet 1 Overall CB - No\n",
      "Doing sheet 2 Overall CB - No\n",
      "Doing sheet 3 Overall CB - No\n",
      "Doing sheet 4 Overall CB - No\n",
      "Doing sheet 5 Overall CB - No\n",
      "Doing sheet 6 Overall CB - Yes\n",
      "Doing sheet 7 Overall CB - No\n",
      "Doing sheet 8 Overall CB - No\n",
      "Doing sheet 9 Overall CB - No\n",
      "Doing sheet 10 Overall CB - No\n",
      "Doing sheet 11 Overall CB - Yes\n",
      "Doing sheet 12 Overall CB - Yes\n",
      "Doing sheet 13 Overall CB - Yes\n",
      "Doing sheet 14 Overall CB - Yes\n",
      "Doing sheet 15 Overall CB - Yes\n",
      "Doing sheet 16 Overall CB - Yes\n",
      "Doing sheet 17 Overall CB - Yes\n",
      "Doing sheet 18 Overall CB - Yes\n",
      "Doing sheet 19 Overall CB - Yes\n",
      "Doing sheet 20 Overall CB - Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/anique/DataspellProjects/anti_bullying/common.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hidden_states = submodule(torch.tensor(torch.tensor(inputs).to(device)))[0][:, -4:, :]\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sessions = get_instagram_sessions()\n",
    "all_comments = [y['content'] for x in sessions for y in x['comments']]\n",
    "# Use the Euler scheduler here instead\n",
    "# scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, max_len=300, truncation=True, padding=True)\n",
    "# pipe.tokenizer.model_max_length = 300\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "apol, comf, stop = get_word_lists()\n",
    "apol, comf, stop = [[x.strip() for x in y] for y in [apol, comf, stop]]\n",
    "\n",
    "# pipe._encode_prompt(apol, device, 1, True, None)\n",
    "# Get the encodings for all the words\n",
    "architype_encodings = []\n",
    "for l in [apol, comf, stop]:\n",
    "    m_l = get_text_embedding(l)\n",
    "    architype_encodings.append(m_l)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Encode the instagram session texts\n",
    "comment_encodings = get_text_embeddings_splitted(all_comments, 256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Compare the encoded comments with the architypes\n",
    "comment_stats = defaultdict(list)\n",
    "for i_c, ce in enumerate(comment_encodings):\n",
    "    for i, arch in enumerate(architype_encodings):\n",
    "        min_cos = min_euc = float('inf')\n",
    "        mean_cos = mean_euc = 0\n",
    "        for arch_i in arch:\n",
    "            cos_dist = cosine(arch_i, ce)\n",
    "            euc_dist = euclidean(arch_i, ce)\n",
    "            min_cos = cos_dist if cos_dist<min_cos else min_cos\n",
    "            min_euc = euc_dist if euc_dist<min_euc else min_euc\n",
    "            mean_cos += cos_dist\n",
    "            mean_euc += euc_dist\n",
    "        mean_euc, mean_cos = mean_euc/len(arch), mean_cos/len(arch)\n",
    "        comment_stats[i_c].append({\n",
    "            'arch': i,\n",
    "            'min_cos': min_cos,\n",
    "            'min_euc': min_euc,\n",
    "            'mean_cos': mean_cos,\n",
    "            'mean_euc': mean_euc\n",
    "        })\n",
    "\n",
    "comment_data = []\n",
    "for i, c in enumerate(all_comments):\n",
    "    comment_datum = {}\n",
    "    for v in comment_stats[i]:\n",
    "        arch = v['arch']\n",
    "        for k1, v1 in v.items():\n",
    "            if k1 == 'arch':\n",
    "                continue\n",
    "            comment_datum[f'{k1}_{arch}'] = v1\n",
    "    comment_datum['comment'] = c\n",
    "    comment_data.append(comment_datum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame(comment_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "      min_cos_0  min_euc_0  mean_cos_0  mean_euc_0  min_cos_1  min_euc_1  \\\n0      0.008197   8.111126    0.015723   11.060664   0.006484   7.213890   \n1      0.012835  10.149008    0.020010   12.557677   0.011332   9.536220   \n2      0.008699   8.355655    0.016143   11.217775   0.006895   7.438455   \n3      0.030127  15.537373    0.038980   17.618789   0.030246  15.569288   \n4      0.056284  21.233501    0.072391   24.053590   0.063283  22.517595   \n...         ...        ...         ...         ...        ...        ...   \n8448   0.007982   8.004100    0.015502   10.974006   0.006066   6.977427   \n8449   0.052711  20.550373    0.069323   23.539519   0.060012  21.930006   \n8450   0.011190   9.472099    0.018511   12.027801   0.009045   8.515697   \n8451   0.036195  17.037941    0.042023   18.333767   0.034464  16.624519   \n8452   0.047816  19.578386    0.061380   22.153772   0.052820  20.579773   \n\n      mean_cos_1  mean_euc_1  min_cos_2  min_euc_2  mean_cos_2  mean_euc_2  \\\n0       0.015913   11.019971   0.008038   8.032723    0.015697   10.946465   \n1       0.020214   12.545956   0.012996  10.213217    0.020110   12.514229   \n2       0.016310   11.169682   0.008333   8.178728    0.015937   11.040151   \n3       0.041280   18.097596   0.022554  13.441738    0.030731   15.621138   \n4       0.074888   24.463892   0.043945  18.759750    0.060874   22.001907   \n...          ...         ...        ...        ...         ...         ...   \n8448    0.015536   10.874890   0.007807   7.916058    0.015386   10.827153   \n8449    0.071812   23.954413   0.037453  17.320288    0.057821   21.422588   \n8450    0.018882   12.050409   0.008673   8.337419    0.016051   11.176107   \n8451    0.042128   18.341152   0.035824  16.946238    0.041034   18.088460   \n8452    0.064226   22.657206   0.037879  17.423515    0.051925   20.330798   \n\n                                                comment  \n0                                               Zaaaayn  \n1                                                 Larry  \n2                                           Niall_Å½Ã›_  \n3      How do u get a gif? I cant aave them to my phone  \n4     Larry, Zayn being sexy, and Niall and Liam doi...  \n...                                                 ...  \n8448  @spartacusfan7125 im still waitin on your \"goo...  \n8449                        Haha right!! @thenameisandy  \n8450  @jovanadragicevic ___¢___¢___¢___¢...  \n8451  ___¢___¢___¢___¢ ja @ichliebejordb...  \n8452             What a classic __¥¥_ @thehughjackman  \n\n[8453 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_cos_0</th>\n      <th>min_euc_0</th>\n      <th>mean_cos_0</th>\n      <th>mean_euc_0</th>\n      <th>min_cos_1</th>\n      <th>min_euc_1</th>\n      <th>mean_cos_1</th>\n      <th>mean_euc_1</th>\n      <th>min_cos_2</th>\n      <th>min_euc_2</th>\n      <th>mean_cos_2</th>\n      <th>mean_euc_2</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.008197</td>\n      <td>8.111126</td>\n      <td>0.015723</td>\n      <td>11.060664</td>\n      <td>0.006484</td>\n      <td>7.213890</td>\n      <td>0.015913</td>\n      <td>11.019971</td>\n      <td>0.008038</td>\n      <td>8.032723</td>\n      <td>0.015697</td>\n      <td>10.946465</td>\n      <td>Zaaaayn</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.012835</td>\n      <td>10.149008</td>\n      <td>0.020010</td>\n      <td>12.557677</td>\n      <td>0.011332</td>\n      <td>9.536220</td>\n      <td>0.020214</td>\n      <td>12.545956</td>\n      <td>0.012996</td>\n      <td>10.213217</td>\n      <td>0.020110</td>\n      <td>12.514229</td>\n      <td>Larry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.008699</td>\n      <td>8.355655</td>\n      <td>0.016143</td>\n      <td>11.217775</td>\n      <td>0.006895</td>\n      <td>7.438455</td>\n      <td>0.016310</td>\n      <td>11.169682</td>\n      <td>0.008333</td>\n      <td>8.178728</td>\n      <td>0.015937</td>\n      <td>11.040151</td>\n      <td>Niall_Å½Ã›_</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.030127</td>\n      <td>15.537373</td>\n      <td>0.038980</td>\n      <td>17.618789</td>\n      <td>0.030246</td>\n      <td>15.569288</td>\n      <td>0.041280</td>\n      <td>18.097596</td>\n      <td>0.022554</td>\n      <td>13.441738</td>\n      <td>0.030731</td>\n      <td>15.621138</td>\n      <td>How do u get a gif? I cant aave them to my phone</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.056284</td>\n      <td>21.233501</td>\n      <td>0.072391</td>\n      <td>24.053590</td>\n      <td>0.063283</td>\n      <td>22.517595</td>\n      <td>0.074888</td>\n      <td>24.463892</td>\n      <td>0.043945</td>\n      <td>18.759750</td>\n      <td>0.060874</td>\n      <td>22.001907</td>\n      <td>Larry, Zayn being sexy, and Niall and Liam doi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8448</th>\n      <td>0.007982</td>\n      <td>8.004100</td>\n      <td>0.015502</td>\n      <td>10.974006</td>\n      <td>0.006066</td>\n      <td>6.977427</td>\n      <td>0.015536</td>\n      <td>10.874890</td>\n      <td>0.007807</td>\n      <td>7.916058</td>\n      <td>0.015386</td>\n      <td>10.827153</td>\n      <td>@spartacusfan7125 im still waitin on your \"goo...</td>\n    </tr>\n    <tr>\n      <th>8449</th>\n      <td>0.052711</td>\n      <td>20.550373</td>\n      <td>0.069323</td>\n      <td>23.539519</td>\n      <td>0.060012</td>\n      <td>21.930006</td>\n      <td>0.071812</td>\n      <td>23.954413</td>\n      <td>0.037453</td>\n      <td>17.320288</td>\n      <td>0.057821</td>\n      <td>21.422588</td>\n      <td>Haha right!! @thenameisandy</td>\n    </tr>\n    <tr>\n      <th>8450</th>\n      <td>0.011190</td>\n      <td>9.472099</td>\n      <td>0.018511</td>\n      <td>12.027801</td>\n      <td>0.009045</td>\n      <td>8.515697</td>\n      <td>0.018882</td>\n      <td>12.050409</td>\n      <td>0.008673</td>\n      <td>8.337419</td>\n      <td>0.016051</td>\n      <td>11.176107</td>\n      <td>@jovanadragicevic ___¢___¢___¢___¢...</td>\n    </tr>\n    <tr>\n      <th>8451</th>\n      <td>0.036195</td>\n      <td>17.037941</td>\n      <td>0.042023</td>\n      <td>18.333767</td>\n      <td>0.034464</td>\n      <td>16.624519</td>\n      <td>0.042128</td>\n      <td>18.341152</td>\n      <td>0.035824</td>\n      <td>16.946238</td>\n      <td>0.041034</td>\n      <td>18.088460</td>\n      <td>___¢___¢___¢___¢ ja @ichliebejordb...</td>\n    </tr>\n    <tr>\n      <th>8452</th>\n      <td>0.047816</td>\n      <td>19.578386</td>\n      <td>0.061380</td>\n      <td>22.153772</td>\n      <td>0.052820</td>\n      <td>20.579773</td>\n      <td>0.064226</td>\n      <td>22.657206</td>\n      <td>0.037879</td>\n      <td>17.423515</td>\n      <td>0.051925</td>\n      <td>20.330798</td>\n      <td>What a classic __¥¥_ @thehughjackman</td>\n    </tr>\n  </tbody>\n</table>\n<p>8453 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_comments, x='min_cos_0', y='min_cos_1', z='min_cos_2', hover_data=['comment'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "fig.write_html('min_cos.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_comments, x='min_euc_0', y='min_euc_1', z='min_euc_2', hover_data=['comment'])\n",
    "fig.write_html('min_euc.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_comments, x='mean_cos_0', y='mean_cos_1', z='mean_cos_2', hover_data=['comment'])\n",
    "fig.write_html('mean_cos.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_comments, x='mean_euc_0', y='mean_euc_1', z='mean_euc_2', hover_data=['comment'])\n",
    "fig.write_html('mean_euc.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
